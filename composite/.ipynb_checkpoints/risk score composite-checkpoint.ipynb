{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "extra-command",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/buckcenter.org/hhuang/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm as cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.utils import resample\n",
    "from sklearn.utils.extmath import cartesian\n",
    "\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import dill\n",
    "import copy\n",
    "import io\n",
    "import os\n",
    "import shutil\n",
    "import tarfile\n",
    "\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import HTML\n",
    "\n",
    "#!pip install -U -q tensorflow-gpu\n",
    "#!pip install -U -q tensorflow-cpu\n",
    "import tensorflow as tf\n",
    "\n",
    "#!pip install -U -q pysurvival\n",
    "from pysurvival.models.semi_parametric import CoxPHModel, NonLinearCoxPHModel\n",
    "from pysurvival.models.non_parametric import KaplanMeierModel\n",
    "from pysurvival.models._coxph import _baseline_functions\n",
    "from pysurvival.utils.display import display_non_parametric, compare_to_actual, display_loss_values, correlation_matrix\n",
    "\n",
    "#!pip install -U -q lifelines\n",
    "from lifelines.statistics import logrank_test\n",
    "from lifelines import CoxPHFitter, KaplanMeierFitter\n",
    "from lifelines.utils import concordance_index\n",
    "\n",
    "##!pip install -U -q PyDrive\n",
    "#from pydrive.auth import GoogleAuth\n",
    "#from pydrive.drive import GoogleDrive\n",
    "##from google.colab import auth, files\n",
    "#from oauth2client.client import GoogleCredentials\n",
    "\n",
    "##Henry\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "tribal-chicago",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_birth</th>\n",
       "      <th>date_assessment</th>\n",
       "      <th>age_assessment</th>\n",
       "      <th>sex</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>BMI</th>\n",
       "      <th>smoking_num</th>\n",
       "      <th>diastolic_blood_pressure</th>\n",
       "      <th>systolic_blood_pressure</th>\n",
       "      <th>smoking_cat</th>\n",
       "      <th>...</th>\n",
       "      <th>E_Incidence_CHD</th>\n",
       "      <th>T_Incidence_CHD</th>\n",
       "      <th>E_Incidence_Stroke</th>\n",
       "      <th>T_Incidence_Stroke</th>\n",
       "      <th>E_Incidence_Composite</th>\n",
       "      <th>T_Incidence_Composite</th>\n",
       "      <th>E_Mortality_W-CVD</th>\n",
       "      <th>T_Mortality_W-CVD</th>\n",
       "      <th>E_Incidence_W-CVD</th>\n",
       "      <th>T_Incidence_W-CVD</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2662726</th>\n",
       "      <td>1962-12-15</td>\n",
       "      <td>2009-12-11</td>\n",
       "      <td>46.990698</td>\n",
       "      <td>Male</td>\n",
       "      <td>Black</td>\n",
       "      <td>23.4924</td>\n",
       "      <td>2.0</td>\n",
       "      <td>82.5</td>\n",
       "      <td>135.5</td>\n",
       "      <td>Current</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>7.301998</td>\n",
       "      <td>False</td>\n",
       "      <td>7.301998</td>\n",
       "      <td>False</td>\n",
       "      <td>7.301998</td>\n",
       "      <td>False</td>\n",
       "      <td>8.178128</td>\n",
       "      <td>False</td>\n",
       "      <td>7.301998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2232224</th>\n",
       "      <td>1940-12-15</td>\n",
       "      <td>2009-05-22</td>\n",
       "      <td>68.433986</td>\n",
       "      <td>Male</td>\n",
       "      <td>White</td>\n",
       "      <td>24.8075</td>\n",
       "      <td>0.0</td>\n",
       "      <td>82.5</td>\n",
       "      <td>158.5</td>\n",
       "      <td>Never</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>3.466190</td>\n",
       "      <td>False</td>\n",
       "      <td>7.857793</td>\n",
       "      <td>True</td>\n",
       "      <td>3.466190</td>\n",
       "      <td>False</td>\n",
       "      <td>8.733923</td>\n",
       "      <td>False</td>\n",
       "      <td>7.857793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4485128</th>\n",
       "      <td>1961-09-15</td>\n",
       "      <td>2008-09-25</td>\n",
       "      <td>47.029029</td>\n",
       "      <td>Female</td>\n",
       "      <td>White</td>\n",
       "      <td>24.0436</td>\n",
       "      <td>2.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>Current</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>8.512153</td>\n",
       "      <td>False</td>\n",
       "      <td>8.512153</td>\n",
       "      <td>False</td>\n",
       "      <td>8.512153</td>\n",
       "      <td>False</td>\n",
       "      <td>9.388283</td>\n",
       "      <td>False</td>\n",
       "      <td>8.512153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4326774</th>\n",
       "      <td>1950-07-15</td>\n",
       "      <td>2009-11-12</td>\n",
       "      <td>59.330445</td>\n",
       "      <td>Male</td>\n",
       "      <td>White</td>\n",
       "      <td>22.7583</td>\n",
       "      <td>2.0</td>\n",
       "      <td>68.5</td>\n",
       "      <td>121.5</td>\n",
       "      <td>Current</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>7.381397</td>\n",
       "      <td>False</td>\n",
       "      <td>7.381397</td>\n",
       "      <td>False</td>\n",
       "      <td>7.381397</td>\n",
       "      <td>False</td>\n",
       "      <td>8.257528</td>\n",
       "      <td>False</td>\n",
       "      <td>7.381397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3467770</th>\n",
       "      <td>1940-06-15</td>\n",
       "      <td>2008-09-15</td>\n",
       "      <td>68.253284</td>\n",
       "      <td>Female</td>\n",
       "      <td>White</td>\n",
       "      <td>23.6488</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>158.0</td>\n",
       "      <td>Never</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>8.539532</td>\n",
       "      <td>False</td>\n",
       "      <td>8.539532</td>\n",
       "      <td>False</td>\n",
       "      <td>8.539532</td>\n",
       "      <td>False</td>\n",
       "      <td>9.415662</td>\n",
       "      <td>False</td>\n",
       "      <td>8.539532</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 88 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        date_birth date_assessment  age_assessment     sex ethnicity      BMI  \\\n",
       "id                                                                              \n",
       "2662726 1962-12-15      2009-12-11       46.990698    Male     Black  23.4924   \n",
       "2232224 1940-12-15      2009-05-22       68.433986    Male     White  24.8075   \n",
       "4485128 1961-09-15      2008-09-25       47.029029  Female     White  24.0436   \n",
       "4326774 1950-07-15      2009-11-12       59.330445    Male     White  22.7583   \n",
       "3467770 1940-06-15      2008-09-15       68.253284  Female     White  23.6488   \n",
       "\n",
       "         smoking_num  diastolic_blood_pressure  systolic_blood_pressure  \\\n",
       "id                                                                        \n",
       "2662726          2.0                      82.5                    135.5   \n",
       "2232224          0.0                      82.5                    158.5   \n",
       "4485128          2.0                      67.0                     94.0   \n",
       "4326774          2.0                      68.5                    121.5   \n",
       "3467770          0.0                      90.0                    158.0   \n",
       "\n",
       "        smoking_cat  ...  E_Incidence_CHD T_Incidence_CHD  E_Incidence_Stroke  \\\n",
       "id                   ...                                                        \n",
       "2662726     Current  ...            False        7.301998               False   \n",
       "2232224       Never  ...             True        3.466190               False   \n",
       "4485128     Current  ...            False        8.512153               False   \n",
       "4326774     Current  ...            False        7.381397               False   \n",
       "3467770       Never  ...            False        8.539532               False   \n",
       "\n",
       "         T_Incidence_Stroke  E_Incidence_Composite  T_Incidence_Composite  \\\n",
       "id                                                                          \n",
       "2662726            7.301998                  False               7.301998   \n",
       "2232224            7.857793                   True               3.466190   \n",
       "4485128            8.512153                  False               8.512153   \n",
       "4326774            7.381397                  False               7.381397   \n",
       "3467770            8.539532                  False               8.539532   \n",
       "\n",
       "         E_Mortality_W-CVD  T_Mortality_W-CVD  E_Incidence_W-CVD  \\\n",
       "id                                                                 \n",
       "2662726              False           8.178128              False   \n",
       "2232224              False           8.733923              False   \n",
       "4485128              False           9.388283              False   \n",
       "4326774              False           8.257528              False   \n",
       "3467770              False           9.415662              False   \n",
       "\n",
       "         T_Incidence_W-CVD  \n",
       "id                          \n",
       "2662726           7.301998  \n",
       "2232224           7.857793  \n",
       "4485128           8.512153  \n",
       "4326774           7.381397  \n",
       "3467770           8.539532  \n",
       "\n",
       "[5 rows x 88 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/df.csv', index_col=0)\n",
    "\n",
    "date_cols = [col for col in df.columns if col.startswith('date_')]\n",
    "df[date_cols] = df[date_cols].apply(pd.to_datetime, format='%Y-%m-%d', axis=0)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "human-small",
   "metadata": {},
   "source": [
    "## GLobal Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "active-natural",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CI(x, sigma2, C=95):\n",
    "  dx = stats.norm.ppf((1 + C/100) / 2, loc = 0, scale = 1) * np.sqrt(sigma2)\n",
    "  return x - dx, x + dx\n",
    "\n",
    "def c_index_bootstrap(T, E, scores, alpha=0.05, n_bootstrap=10, random_state=None):\n",
    "  np.random.seed(random_state)\n",
    "\n",
    "  c_index = concordance_index(T, -scores, E)\n",
    "  \n",
    "  stacked_arr = np.stack((T, -scores, E), axis=-1)\n",
    "  c_idx_boot = np.array([concordance_index(*resample(stacked_arr).T) for _ in range(n_bootstrap)])\n",
    "\n",
    "  c_index_boot = c_idx_boot.mean()\n",
    "  sigma2 = c_idx_boot.var()\n",
    "  CI_lower = np.quantile(c_idx_boot, alpha/2)\n",
    "  CI_upper = np.quantile(c_idx_boot, 1 - alpha/2)\n",
    "\n",
    "  return c_index, c_index_boot, sigma2, CI_lower, CI_upper\n",
    "\n",
    "def c_index_jackknife(T, E, scores, alpha=0.05):\n",
    "\n",
    "  c_index = concordance_index(T, -scores, E)\n",
    "  \n",
    "  stacked_arr = np.stack((T, -scores, E), axis=-1)\n",
    "  n = stacked_arr.shape[0]\n",
    "  c_idx_jack = np.array([concordance_index(*np.delete(stacked_arr, i, axis=0).T) for i in range(n)])\n",
    "  \n",
    "  c_index_jack = c_idx_jack.mean()\n",
    "  sigma2 = (n-1) * c_idx_jack.var()\n",
    "  CI_lower, CI_upper = CI(c_index_jack, sigma2, C=95)\n",
    "\n",
    "  return c_index, c_index_jack, sigma2, CI_lower, CI_upper\n",
    "  \n",
    "def c_diff_bootstrap(T, E, scores1, scores2, alpha=0.05, n_bootstrap=10, random_state=None):\n",
    "\n",
    "  np.random.seed(random_state)\n",
    "  c1 = concordance_index(T, -scores1, E)\n",
    "  c2 = concordance_index(T, -scores2, E)\n",
    "  dc = c1 - c2\n",
    "\n",
    "  stacked_arr = np.stack((T, -scores1, -scores2, E), axis=-1)\n",
    "    \n",
    "  def c_diff(data):\n",
    "    sample = resample(data)\n",
    "    return concordance_index(*sample[:, [0,1,3]].T) - concordance_index(*sample[:, [0,2,3]].T)\n",
    "\n",
    "  dc_sample = np.array([c_diff(stacked_arr) for _ in range(n_bootstrap)])\n",
    "\n",
    "  dc_boot = dc_sample.mean()\n",
    "  sigma2 = dc_sample.var()\n",
    "  CI_lower = np.quantile(dc_sample, alpha/2)\n",
    "  CI_upper = np.quantile(dc_sample, 1 - alpha/2)\n",
    "\n",
    "  return dc, dc_boot, sigma2, CI_lower, CI_upper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subjective-forward",
   "metadata": {},
   "source": [
    "## Deepsurv Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "clean-alarm",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepSurv:\n",
    "\n",
    "  def __init__(self, structure, input_dim, n_outcomes=1):\n",
    "    \n",
    "    self.structure = structure\n",
    "    self.input_dim = input_dim\n",
    "    self.n_outcomes = n_outcomes\n",
    "    self._trained = False\n",
    "\n",
    "    self._TRAIN_BUF = 500000\n",
    "    self._TEST_BUF = 50000\n",
    "    \n",
    "    self.model = tf.keras.Sequential()\n",
    "    input_dim = self.input_dim\n",
    "    \n",
    "    for layer in self.structure['inner_layers']:\n",
    "      activ = layer.setdefault('activation', 'relu')\n",
    "      if activ == 'selu':\n",
    "        layer['initialization'] = 'lecun_uniform'\n",
    "      else:\n",
    "        layer.setdefault('initialization', 'glorot_uniform')\n",
    "      self.model.add(tf.keras.layers.Dense(layer['num_units'], \n",
    "                                           input_shape=(input_dim,),\n",
    "                                           activation=activ, \n",
    "                                           kernel_initializer=layer['initialization'],\n",
    "                                           kernel_regularizer=tf.keras.regularizers.l2(layer.setdefault('l2_reg', 1e-4))))\n",
    "      if layer.setdefault('batch_norm', True):\n",
    "        self.model.add(tf.keras.layers.BatchNormalization())\n",
    "      if layer['dropout'] > 0:\n",
    "        do = layer.setdefault('dropout', 0.1)\n",
    "        self.model.add(tf.keras.layers.AlphaDropout(do) if activ == 'selu' else tf.keras.layers.Dropout(do))\n",
    "      \n",
    "      input_dim = layer['num_units']\n",
    "\n",
    "    self.model.add(tf.keras.layers.Dense(n_outcomes, input_dim=input_dim, activation='linear',\n",
    "                                         use_bias=self.structure['output_layer'].setdefault('use_bias', True),\n",
    "                                         kernel_initializer=layer.setdefault('initialization', 'glorot_uniform'),\n",
    "                                         kernel_regularizer=tf.keras.regularizers.l2(self.structure['output_layer'].setdefault('l2_reg', 1e-4))))\n",
    "\n",
    "  def is_structure(self, structure):\n",
    "      if len(self.structure['inner_layers']) != len(structure['inner_layers']):\n",
    "        return False\n",
    "      \n",
    "      for key, value in structure['output_layer'].items():\n",
    "        if self.structure['output_layer'][key] != value:\n",
    "          return False\n",
    "      \n",
    "      for i, layer in enumerate(structure['inner_layers']):\n",
    "        for key, value in layer.items():\n",
    "          if callable(value):\n",
    "            if not callable(self.structure['inner_layers'][i][key]):\n",
    "              return False\n",
    "            if self.structure['inner_layers'][i][key].__code__.co_code != value.__code__.co_code:\n",
    "              return False\n",
    "          else:\n",
    "            if self.structure['inner_layers'][i][key] != value:\n",
    "              return False\n",
    "      \n",
    "      return True\n",
    "\n",
    "  @staticmethod\n",
    "  def _c_index(E, T, scores):\n",
    "    return concordance_index(T, -scores, E)\n",
    "\n",
    "  @staticmethod\n",
    "  def _get_ith_col(tensor, i):\n",
    "    return tf.reshape(tensor[:, i], (-1, 1))\n",
    "\n",
    "  @staticmethod\n",
    "  def _compute_loss_single_outcome(scores, risk, fail, Efron_coef, Efron_ones):\n",
    "    n_fails = tf.reduce_sum(fail)\n",
    "\n",
    "    numerator = tf.reduce_sum(tf.linalg.matmul(fail, scores))\n",
    "    \n",
    "    risk_scores = tf.linalg.matmul(risk, tf.exp(scores))\n",
    "    fail_scores = tf.linalg.matmul(fail, tf.exp(scores))\n",
    "    Efron_risk = Efron_ones * risk_scores\n",
    "    Efron_fail = Efron_coef * fail_scores\n",
    "    denominator = tf.reduce_sum(tf.math.log(Efron_risk - Efron_fail + (1-Efron_ones)))\n",
    "\n",
    "    return - (numerator-denominator) / n_fails\n",
    "\n",
    "  def _get_loss_fn(self):\n",
    "    def _compute_loss(scores, risk, fail, Efron_coef, Efron_ones):\n",
    "      loss = 0\n",
    "      for i in range(self.n_outcomes):\n",
    "        loss += self._compute_loss_single_outcome(self._get_ith_col(scores, i),\n",
    "                                                  risk[i], fail[i],\n",
    "                                                  Efron_coef[i], Efron_ones[i])\n",
    "      return loss\n",
    "    return _compute_loss if self.eager else tf.function(_compute_loss)\n",
    "\n",
    "  def _get_apply_grad_fn(self, loss_fn, optimizer):\n",
    "    def apply_gradients(X, risk, fail, Efron_coef, Efron_ones):\n",
    "      with tf.GradientTape() as tape:\n",
    "        loss = loss_fn(self.model(X, training=True), risk, fail, Efron_coef, Efron_ones)\n",
    "      gradients = tape.gradient(loss, self.model.trainable_variables)\n",
    "      optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
    "      return loss, gradients\n",
    "    return apply_gradients if self.eager else tf.function(apply_gradients)\n",
    "\n",
    "  def _get_Efron(self, fail, fixed_hdim=None):\n",
    "    Efron_coef = ()\n",
    "    Efron_ones = ()\n",
    "    for i in range(self.n_outcomes):\n",
    "      n_fail_per_time = fail[i].sum(axis=1).astype(np.int32)\n",
    "      h_dim = n_fail_per_time.max() if fixed_hdim is None else fixed_hdim\n",
    "      if h_dim < n_fail_per_time.max():\n",
    "        raise ValueError('DeepSurv::_get_Efron: the fixed horizontal direction ({}) is strictly smaller than the maximum number of events per time ({})'.format(h_dim, n_fail_per_time.max()))\n",
    "      \n",
    "      Efron_coef += (np.stack([np.pad(np.arange(d)/d, (0, h_dim-d)) for d in n_fail_per_time], axis=0).astype(np.float32),)\n",
    "      Efron_ones += (np.stack([np.pad(np.ones(d), (0, h_dim-d)) for d in n_fail_per_time], axis=0).astype(np.float32),)\n",
    "    \n",
    "    return Efron_coef, Efron_ones\n",
    "  \n",
    "  def _get_risk_fail(self, E, T, fixed_vdim=None):\n",
    "    times = ()\n",
    "    risk = ()\n",
    "    fail = ()\n",
    "    pad = fixed_vdim - len(times) if fixed_vdim is not None else 0\n",
    "    if pad < 0:\n",
    "      raise ValueError('DeepSurv::_get_risk_fail: the fixed vertical direction is strictly smaller than the number of times with at least an event')\n",
    "\n",
    "    for i in range(self.n_outcomes):\n",
    "      Ei = E[i].numpy()\n",
    "      Ti = T[i].numpy()\n",
    "      ti = np.unique(Ti[Ei.astype(np.bool)])\n",
    "      times += (ti,)\n",
    "      risk += (np.pad((ti.reshape(-1, 1) <= Ti).astype(np.float32),\n",
    "                      pad_width=((0, pad), (0, 0))),)\n",
    "      fail += (np.pad((ti.reshape(-1, 1) == Ti).astype(np.float32) * Ei,\n",
    "                      pad_width=((0, pad), (0, 0))),)\n",
    "\n",
    "    return times, risk, fail\n",
    "  \n",
    "  def fit(self,\n",
    "          X_train, E_train, T_train,\n",
    "          X_test, E_test, T_test,\n",
    "          optimizer=tf.keras.optimizers.Adam(),\n",
    "          n_epochs=20, lr=1e-3,\n",
    "          batch_size=None, batch_n_events=None,\n",
    "          eager=True, opt_memory=False,\n",
    "          evaluation_prms={}):\n",
    "    \n",
    "    self.eager = eager\n",
    "    self.opt_config = optimizer.get_config()\n",
    "    self.opt_config['learning_rate'] = lr\n",
    "    self.n_epochs = [n_epochs] if not self._trained else self.n_epochs + [n_epochs]\n",
    "    \n",
    "    E_train_ = E_train if isinstance(E_train, tuple) else (E_train,)\n",
    "    T_train_ = T_train if isinstance(T_train, tuple) else (T_train,)\n",
    "    E_test_ = E_test if isinstance(E_test, tuple) else (E_test,)\n",
    "    T_test_ = T_test if isinstance(T_test, tuple) else (T_test,)\n",
    "\n",
    "    if batch_size is not None:\n",
    "      batch_size_ = batch_size\n",
    "    elif batch_n_events is not None:\n",
    "      perc_events = min([E.sum() / E.shape[0] for E in E_train_])\n",
    "      batch_size_ = int(np.ceil(batch_n_events / perc_events))\n",
    "      print('Batch size automatically computed as {} samples'.format(batch_size_))\n",
    "    else:\n",
    "      batch_size_ = X_train.shape[0]\n",
    "    \n",
    "    steps_train_ = X_train.shape[0] // batch_size_\n",
    "    steps_test_ = X_test.shape[0] // batch_size_\n",
    "\n",
    "    if not self._trained:\n",
    "      self.batch_size = []\n",
    "      self.steps_train = []\n",
    "      self.steps_test = []\n",
    "    \n",
    "    self.batch_size += [batch_size_]\n",
    "    self.steps_train += [steps_train_]\n",
    "    self.steps_test += [steps_test_]\n",
    "    \n",
    "    print('During training: {} steps per epoch'.format(steps_train_))\n",
    "\n",
    "    dataset_train = (tf.data.Dataset.from_tensor_slices((X_train.to_numpy().astype(np.float32), E_train_, T_train_))\n",
    "                     .shuffle(self._TRAIN_BUF, reshuffle_each_iteration=self.eager or not opt_memory)\n",
    "                     .batch(batch_size_, drop_remainder=True))\n",
    "    dataset_test = (tf.data.Dataset.from_tensor_slices((X_test.to_numpy().astype(np.float32), E_test_, T_test_))\n",
    "                    .shuffle(self._TEST_BUF, reshuffle_each_iteration=self.eager or not opt_memory)\n",
    "                    .batch(batch_size_, drop_remainder=True))\n",
    "    \n",
    "    # define apply_gradient functions\n",
    "    apply_gradients = self._get_apply_grad_fn(self._get_loss_fn(), \n",
    "                                              optimizer.from_config(self.opt_config))\n",
    "    \n",
    "    if not hasattr(self, 'loss_train'):\n",
    "      self.loss_train = np.array([])\n",
    "    if not hasattr(self, 'loss_epoch_train'):\n",
    "      self.loss_epoch_train = np.array([])\n",
    "    \n",
    "    # batch data for (static) matrix sizes\n",
    "    if self.eager:\n",
    "      vdim_train = None\n",
    "      hdim_train = None\n",
    "    elif opt_memory:\n",
    "      vdim_train = 0\n",
    "      hdim_train = 0\n",
    "      for _, E, T in dataset_train:\n",
    "        times, _, fail = self._get_risk_fail(E, T)\n",
    "        for i in range(self.n_outcomes):\n",
    "          vdim_train = max(vdim_train, len(times[i]))\n",
    "          hdim_train = max(hdim_train, fail[i].sum(axis=1).astype(np.int32).max())\n",
    "    else:\n",
    "      vdim_train = batch_size_\n",
    "      hdim_train = batch_size_\n",
    "      \n",
    "    # initilise evaluation parameters\n",
    "    evaluation_prms.setdefault('c_train', True)\n",
    "    evaluation_prms.setdefault('c_test', True)\n",
    "    evaluation_prms.setdefault('loss_test', True)\n",
    "    \n",
    "    if evaluation_prms['c_train'] and not hasattr(self, 'c_train'):\n",
    "      self.c_train = [[]]*self.n_outcomes\n",
    "    if evaluation_prms['c_test'] and not hasattr(self, 'c_test'):\n",
    "      self.c_test = [[]]*self.n_outcomes\n",
    "    \n",
    "    if evaluation_prms['loss_test']:\n",
    "      loss_fn_test = self._get_loss_fn()\n",
    "      if not hasattr(self, 'loss_test'):\n",
    "        self.loss_test = np.array([])\n",
    "      if not hasattr(self, 'loss_epoch_test'):\n",
    "        self.loss_epoch_test = np.array([])\n",
    "      \n",
    "      if self.eager:\n",
    "        vdim_test = None\n",
    "        hdim_test = None\n",
    "      elif opt_memory:\n",
    "        vdim_test = 0\n",
    "        hdim_test = 0\n",
    "        for _, E, T in dataset_test:\n",
    "          times, _, fail = self._get_risk_fail(E, T)\n",
    "          for i in range(self.n_outcomes):\n",
    "            vdim_test = max(vdim_test, len(times[i]))\n",
    "            hdim_test = max(hdim_test, fail[i].sum(axis=1).astype(np.int32).max())\n",
    "      else:\n",
    "        vdim_test = batch_size_\n",
    "        hdim_test = batch_size_\n",
    "    \n",
    "    print('\\nTraining...')\n",
    "    tot_epochs = sum(self.n_epochs)\n",
    "    for epoch in range(tot_epochs-n_epochs, tot_epochs):\n",
    "      start_time = datetime.now()\n",
    "      losses_epoch = np.array([])\n",
    "      str_epoch = 'Epoch {}/{}:'.format(epoch + 1, tot_epochs)\n",
    "      print('{} {}'.format(str_epoch, self._load_bar(0, steps_train_)), end='')\n",
    "      for i, train_batch in enumerate(dataset_train):\n",
    "        X_batch = train_batch[0]\n",
    "        E_batch = train_batch[1]\n",
    "        T_batch = train_batch[2]\n",
    "        _, risk, fail = self._get_risk_fail(E_batch, T_batch, fixed_vdim=vdim_train)\n",
    "        Efron_coef, Efron_ones = self._get_Efron(fail, fixed_hdim=hdim_train)\n",
    "        loss, _ = apply_gradients(X_batch,\n",
    "                                  risk, fail,\n",
    "                                  Efron_coef, Efron_ones)\n",
    "        losses_epoch = np.append(losses_epoch, loss.numpy())\n",
    "        print('\\r{} {} Learning time!'.format(str_epoch, self._load_bar(i+1, steps_train_)), end='')\n",
    "      \n",
    "      self.loss_train = np.append(self.loss_train, losses_epoch)\n",
    "      self.loss_epoch_train = np.append(self.loss_epoch_train, losses_epoch.mean())\n",
    "      full_bar = self._load_bar(1, 1)\n",
    "      print('\\r{} {}'.format(str_epoch, full_bar), end='')\n",
    "      \n",
    "      if evaluation_prms['loss_test']:\n",
    "        print('\\r{} {} Computing loss function on Test Batches...'.format(str_epoch, full_bar), end='')\n",
    "        losses_epoch = np.array([])\n",
    "        for test_batch in dataset_test:\n",
    "          X_batch = test_batch[0]\n",
    "          E_batch = test_batch[1]\n",
    "          T_batch = test_batch[2]\n",
    "          _, risk, fail = self._get_risk_fail(E_batch, T_batch, fixed_vdim=vdim_test)\n",
    "          Efron_coef, Efron_ones = self._get_Efron(fail, fixed_hdim=hdim_test)\n",
    "          losses_epoch = np.append(losses_epoch,\n",
    "                                   loss_fn_test(self.model(X_batch),\n",
    "                                                risk, fail,\n",
    "                                                Efron_coef, Efron_ones).numpy())\n",
    "          \n",
    "        self.loss_test = np.append(self.loss_test, losses_epoch)\n",
    "        self.loss_epoch_test = np.append(self.loss_epoch_test, losses_epoch.mean())\n",
    "\n",
    "      if evaluation_prms['c_train']:\n",
    "        scores = self.model(X_train.to_numpy().astype(np.float32)).numpy()\n",
    "        for i in range(self.n_outcomes):\n",
    "          print('\\r{} {} Computing c_index for outcome {} on Train Set...'.format(str_epoch, full_bar, i+1), end='')\n",
    "          self.c_train[i] = self.c_train[i] + [self._c_index(E_train_[i], T_train_[i], scores[:, i])]\n",
    "      if evaluation_prms['c_test']:\n",
    "        scores = self.model(X_test.to_numpy().astype(np.float32)).numpy()\n",
    "        for i in range(self.n_outcomes):\n",
    "          print('\\r{} {} Computing c_index for outcome {} on Test Set...'.format(str_epoch, full_bar, i+1), end='')\n",
    "          self.c_test[i] = self.c_test[i] + [self._c_index(E_test_[i], T_test_[i], scores[:, i])]\n",
    "\n",
    "      info_str = '\\r{}'.format(str_epoch)\n",
    "      if evaluation_prms['c_train']:\n",
    "        info_str += 'Train set c_index: ('\n",
    "        for i in range(self.n_outcomes): \n",
    "          info_str += '{:.6f} '.format(self.c_train[i][-1])\n",
    "        info_str += ') -- '\n",
    "      if evaluation_prms['c_test']:\n",
    "        info_str += 'Test set c_index: ('\n",
    "        for i in range(self.n_outcomes): \n",
    "          info_str += '{:.6f} '.format(self.c_test[i][-1])\n",
    "        info_str += ') -- '\n",
    "      info_str += 'time elapse for current epoch {}'.format(datetime.now() - start_time)\n",
    "      print(info_str)\n",
    "      \n",
    "    print('Computing Baselines (using PySurvival function)...'.format())\n",
    "    self.times, self.baseline_hazard, self.baseline_survival = self._compute_baseline(X_train, T_train_, E_train_, PySurv=True)\n",
    "\n",
    "    print('Training finished')\n",
    "    self._trained = True\n",
    "\n",
    "    print('\\nEvaluation:')\n",
    "    evaluation_prms.setdefault('n_bootstrap', 10)\n",
    "    self.evaluate(X_train, E_train_, T_train_,\n",
    "                  X_test, E_test_, T_test_,\n",
    "                  prms=evaluation_prms)\n",
    "    \n",
    "  def evaluate(self, \n",
    "               X_train, E_train, T_train,\n",
    "               X_test, E_test, T_test, \n",
    "               prms):\n",
    "    \n",
    "    n_bootstrap = prms.get('n_bootstrap', 100)\n",
    "    c_train = prms.get('c_train', True)\n",
    "    c_test = prms.get('c_test', True)\n",
    "    loss_test = prms.get('loss_test', True)\n",
    "\n",
    "    # Computation of c-index on Train and Test set with errors\n",
    "    if c_train or c_test:\n",
    "      print('Bootstrap computation of c-index with {} bootstraps...'.format(n_bootstrap))\n",
    "      c_index_names = ['c_index', 'c_index_boot', 'sigma2', 'CI_lower', 'CI_upper']\n",
    "    if c_train:\n",
    "      self.c_index_train = ()\n",
    "      scores = self.predict_risk(X_train)\n",
    "      for i in range(self.n_outcomes):\n",
    "        c_index_train = c_index_bootstrap(T_train[i], E_train[i], scores[:, i], n_bootstrap=n_bootstrap)\n",
    "        self.c_index_train += (dict(zip(c_index_names, c_index_train)),)\n",
    "        print('c-index (train set, outcome {}) : {:.4f} ({:.4f}) CI({:.4f}, {:.4f})'.format(\n",
    "            i+1,\n",
    "            self.c_index_train[i]['c_index'], np.sqrt(self.c_index_train[i]['sigma2']), \n",
    "            self.c_index_train[i]['CI_lower'], self.c_index_train[i]['CI_upper']))\n",
    "    if c_test:\n",
    "      self.c_index_test = ()\n",
    "      scores = self.predict_risk(X_test)\n",
    "      for i in range(self.n_outcomes):\n",
    "        c_index_test = c_index_bootstrap(T_test[i], E_test[i], scores[:, i], n_bootstrap=n_bootstrap)\n",
    "        self.c_index_test += (dict(zip(c_index_names, c_index_test)),)\n",
    "        print('c-index (test set, outcome {}) : {:.4f} ({:.4f}) CI({:.4f}, {:.4f})'.format(\n",
    "            i+1,\n",
    "            self.c_index_test[i]['c_index'], np.sqrt(self.c_index_test[i]['sigma2']), \n",
    "            self.c_index_test[i]['CI_lower'], self.c_index_test[i]['CI_upper']))\n",
    "\n",
    "    # Plot of Loss on Train and Test set\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(15, 8))\n",
    "\n",
    "    x_train = np.array([])\n",
    "    x_test = np.array([])\n",
    "    tot_epochs = sum(self.n_epochs)\n",
    "    for i, e in enumerate(self.n_epochs):\n",
    "      x_train = np.append(x_train, np.sum(self.n_epochs[:i]) + (np.arange(self.n_epochs[i]*self.steps_train[i])+1)/self.steps_train[i])\n",
    "      x_test = np.append(x_test, np.sum(self.n_epochs[:i]) + (np.arange(self.n_epochs[i]*self.steps_test[i])+1)/self.steps_test[i])\n",
    "    \n",
    "    for x in np.cumsum(self.n_epochs[:-1]):\n",
    "      ax.axvline(x, ymin=0, ymax=1, color='orange', alpha=0.8, lw=1.5, ls=':')\n",
    "    ax.plot(x_train, self.loss_train, color='blue', alpha=0.3, lw=1, label='Loss Values on Train Batch')\n",
    "    if loss_test and hasattr(self, 'loss_test'):\n",
    "      ax.plot(x_test, self.loss_test, color='red', alpha=0.3, lw=2, label='Loss values on Test Batch')\n",
    "    ax.plot(np.arange(tot_epochs)+1, self.loss_epoch_train,\n",
    "            color='blue', alpha=1, lw=3, label='Average Loss Value of Train Batch during Epoch')\n",
    "    if loss_test and hasattr(self, 'loss_test'):\n",
    "      ax.plot(np.arange(tot_epochs)+1, self.loss_epoch_test,\n",
    "              color='red', alpha=1, lw=3, label='Average Loss Value of Test Batch during Epoch')\n",
    "    ax.set_xlabel('Number of eopochs', fontsize=12)\n",
    "    ax.legend(fontsize=12)\n",
    "    ax.set_title('Loss function values', fontsize=14)\n",
    "    fig.show()\n",
    "\n",
    "    # Plots of c-index on Train and Test set\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=self.n_outcomes, figsize=(10*self.n_outcomes, 8))\n",
    "    for i, ax in enumerate(np.array(axs).reshape(-1)):\n",
    "      for x in np.cumsum(self.n_epochs[:-1]):\n",
    "        ax.axvline(x, ymin=0, ymax=1, color='orange', alpha=0.8, lw=1.5, ls=':')\n",
    "      if c_train and hasattr(self, 'c_train'):\n",
    "        ax.plot(np.arange(tot_epochs)+1, self.c_train[i], color='blue', label='c-index on Train set')\n",
    "      if c_test and hasattr(self, 'c_test'):\n",
    "        ax.plot(np.arange(tot_epochs)+1, self.c_test[i], color='red', label='c-index on Test set')\n",
    "      ax.set_xlabel('Number of epochs', fontsize=12)\n",
    "      ax.legend(fontsize=12)\n",
    "      ax.set_title('C-index for outcome {}'.format(i+1), fontsize=14)\n",
    "    fig.show()\n",
    "    \n",
    "    # Plots of Survival Curves with Kaplan-Meier\n",
    "    if prms.get('check_KM', True):\n",
    "      self.check_KM(X_test, E_test, T_test)\n",
    "\n",
    "    return\n",
    "  \n",
    "  def _compute_baseline(self, X, T, E, PySurv=True):\n",
    "    scores = self.predict_risk(X)\n",
    "    times = ()\n",
    "    baseline_hazard = ()\n",
    "    baseline_survival = ()\n",
    "    if PySurv:\n",
    "      for i in range(self.n_outcomes):\n",
    "        desc_order = np.argsort(-T[i])\n",
    "        baselines = _baseline_functions(scores[:, i][desc_order],\n",
    "                                        T[i][desc_order], E[i][desc_order])\n",
    "        times += (np.array(baselines[0]),)\n",
    "        baseline_hazard += (np.array(baselines[1]),)\n",
    "        baseline_survival += (np.array(baselines[2]),)\n",
    "    else:\n",
    "      times, risk, fail = self._get_risk_fail(E, T)\n",
    "      for i in range(self.n_outcomes):\n",
    "        risk_scores = np.dot(risk[i], scores[:, i])\n",
    "        baseline_hazard += (fail[i].sum(axis=1) / risk_scores,)\n",
    "        baseline_survival = (np.exp(-np.cumsum(baseline_hazard)),)\n",
    "    \n",
    "    return times, baseline_hazard, baseline_survival\n",
    "  \n",
    "  def predict_risk(self, X):\n",
    "    if isinstance(X, pd.DataFrame) or isinstance(X, pd.Series):\n",
    "      Xp = X.to_numpy()\n",
    "    else:\n",
    "      Xp = X\n",
    "    return np.exp(self.model(Xp.astype(np.float32)).numpy())\n",
    "    \n",
    "  def predict_hazard_survival(self, X, t=None, outcomes=None):\n",
    "    scores = self.predict_risk(X)\n",
    "    times = ()\n",
    "    h_t = ()\n",
    "    S_t = ()\n",
    "\n",
    "    out = range(self.n_outcomes) if outcomes is None else outcomes\n",
    "    for i in out:\n",
    "      times += (self.times[i],)\n",
    "      if t is not None:\n",
    "        time_bin = np.digitize(t, self.times[i])\n",
    "        h_t += (self.baseline_hazard[i][time_bin] * scores[:, i],)\n",
    "        S_t += (np.power(self.baseline_survival[i][time_bin], scores[:, i]),)\n",
    "      else:\n",
    "        h_t += (self.baseline_hazard[i] * scores[:, i].reshape(-1, 1),)\n",
    "        S_t += (np.power(self.baseline_survival[i], scores[:, i].reshape(-1, 1)),)\n",
    "\n",
    "    return times, h_t, S_t\n",
    "\n",
    "  def survival_curve(self, X):\n",
    "    times, _, S_t = self.predict_hazard_survival(X)\n",
    "    return times, tuple(S_t_i.mean(axis=0) for S_t_i in S_t)\n",
    "\n",
    "  @staticmethod\n",
    "  def _KaplanMeier(E, T, alpha=0.95, method='PySurv'):\n",
    "    if method == 'PySurv':\n",
    "      km = KaplanMeierModel()\n",
    "      km.fit(T, E, alpha=alpha)\n",
    "      return km.times, km.survival, km.survival_ci_lower, km.survival_ci_upper\n",
    "    elif method == 'Lifelines':\n",
    "      km = KaplanMeierFitter(alpha=1-alpha)\n",
    "      km.fit(T, E)\n",
    "      CI = km.confidence_interval_['KM_estimate_lower_{}'.format(alpha)].to_numpy(), km.confidence_interval_['KM_estimate_upper_{}'.format(alpha)].to_numpy()\n",
    "      return (km.timeline, km.survival_function_.to_numpy().flatten()) + CI\n",
    "    else:\n",
    "      times = np.unique(T[E])\n",
    "      risk = (times.reshape(-1, 1) <= T).astype(np.float64)\n",
    "      fail = (times.reshape(-1, 1) == T).astype(np.float64) * E\n",
    "      log_s = np.log(1. - fail.sum(axis=1)/risk.sum(axis=1))\n",
    "      S = np.exp(np.cumsum(log_s))\n",
    "      return times, S\n",
    "\n",
    "  def check_KM(self, X, E, T):\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=self.n_outcomes, figsize=(10*self.n_outcomes, 8))  \n",
    "    \n",
    "    print('Computing Survival Curves...')\n",
    "    times, S_t = self.survival_curve(X)\n",
    "    \n",
    "    print('Computing Kaplan_Meier Curves with PySurvival routine', end=' ')\n",
    "    for i, ax in enumerate(np.array(axs).reshape(-1)):\n",
    "      km_times, km_survival, km_lower, km_upper = self._KaplanMeier(E[i], T[i], method='PySurv')\n",
    "      ax.plot(km_times, km_survival, color='red', lw=2, label='Kaplan-Meier')\n",
    "      ax.fill_between(km_times, km_lower, km_upper,\n",
    "                      facecolor='red', alpha=0.3)\n",
    "      ax.plot(times[i], S_t[i], color='blue', lw=2, label='DeepSurv')\n",
    "      ax.tick_params(axis='both', which='major', labelsize=12)\n",
    "      ax.set_xlabel('time (years)', fontsize=12)\n",
    "      ax.set_title('Kaplan-Meier vs DeepSurv (Survival Curve) for outcome {}'.format(i+1), fontsize=14)\n",
    "      ax.legend(fontsize=12)\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "  @staticmethod\n",
    "  def _load_bar(n, N):\n",
    "    return '[{}{}]'.format('='*int(np.floor(20*n/N)), ' '*int(np.ceil(20*(1-n/N))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "purple-guide",
   "metadata": {},
   "source": [
    "## RISKSCORE class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "lightweight-coordinator",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:87: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:95: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:87: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:95: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<ipython-input-6-e4ac01edf08c>:87: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if na_replace is 'fit_transform':\n",
      "<ipython-input-6-e4ac01edf08c>:95: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif na_replace is 'transform':\n"
     ]
    }
   ],
   "source": [
    "class RiskScore:\n",
    "  \n",
    "  def __init__(self, df,\n",
    "               event_type='Incidence', disease='Composite',\n",
    "               #features=model_feat,\n",
    "               #log_features=log_feat, NA_features=NA_feat, cat_features=cat_feat,\n",
    "               features=[],\n",
    "               log_features=[], NA_features=[], cat_features=[],\n",
    "               secondary_outcomes=[{'event_type': 'Incidence', 'disease': 'Cancer'},\n",
    "                                   {'event_type': 'Mortality', 'disease': 'All'}],\n",
    "               test_size=0.1, data_split_random_state=None,\n",
    "               verb=1):\n",
    "    \n",
    "    #import pdb; pdb.set_trace()\n",
    "\n",
    "    self.event_type = event_type\n",
    "    self.disease = disease\n",
    "    self.secondary_outcomes = secondary_outcomes\n",
    "    \n",
    "    Ecol = 'E_{}_{}'.format(self.event_type, self.disease)\n",
    "    Tcol = 'T_{}_{}'.format(self.event_type, self.disease)\n",
    "\n",
    "    Ecol_II = tuple('E_{}_{}'.format(outcome['event_type'], outcome['disease']) for outcome in self.secondary_outcomes)\n",
    "    Tcol_II = tuple('T_{}_{}'.format(outcome['event_type'], outcome['disease']) for outcome in self.secondary_outcomes)\n",
    "\n",
    "    self.feat = features\n",
    "    self.log_feat = [f for f in self.feat if f in log_features]\n",
    "    self.NA_feat = [f for f in self.feat if f in NA_features]\n",
    "    self.cat_feat = [f for f in self.feat if f in cat_features]\n",
    "    rename_col_f = lambda col: 'log_' + col if col in self.log_feat else col\n",
    "    \n",
    "    # Train Test set\n",
    "    index_train, index_test = train_test_split(df.index, test_size=test_size, random_state=data_split_random_state)\n",
    "    self.X_train = self.preprocess_data(df.loc[index_train], dummies='fit_transform', scale='fit_transform', na_replace='fit_transform')\n",
    "    self.X_test = self.preprocess_data(df.loc[index_test], dummies='transform', scale='transform', na_replace='transform')\n",
    "    self.feat_X = self.X_train.columns\n",
    "\n",
    "    ## Get E, T\n",
    "    self.E_train = df.loc[self.X_train.index, Ecol].values\n",
    "    self.T_train = df.loc[self.X_train.index, Tcol].values\n",
    "    self.E_test = df.loc[self.X_test.index, Ecol].values\n",
    "    self.T_test = df.loc[self.X_test.index, Tcol].values\n",
    "\n",
    "    self.E_train_II = tuple(df.loc[self.X_train.index, E_II].values for E_II in Ecol_II)\n",
    "    self.T_train_II = tuple(df.loc[self.X_train.index, T_II].values for T_II in Tcol_II)\n",
    "    self.E_test_II = tuple(df.loc[self.X_test.index, E_II].values for E_II in Ecol_II)\n",
    "    self.T_test_II = tuple(df.loc[self.X_test.index, T_II].values for T_II in Tcol_II)\n",
    "\n",
    "    # Age bins\n",
    "    n_groups_age = 10\n",
    "    self.df_age_group, self.age_bins = pd.qcut(df.loc[:, 'age_assessment'], n_groups_age, labels=False, retbins=True)\n",
    "    self.df_age_group.rename('age_group', inplace=True)\n",
    "    \n",
    "    # # Baseline Populations\n",
    "    # baseline_vars = ['sex', 'ethnicity', 'baseline_CVD', 'baseline_Cancer', 'baseline_Hypertension', 'baseline_Diabetes', 'med_cholesterol']\n",
    "    # unique_values = tuple(df[var].dropna().unique() for var in baseline_vars)\n",
    "    # all_combn = pd.core.reshape.util.cartesian_product(unique_values)\n",
    "    # self.df_baseline_pop = (pd.DataFrame({baseline_vars[i]: arr for i, arr in enumerate(all_combn)})\n",
    "    #                         .assign(pop_group=np.arange(len(all_combn[0]))))\n",
    "    # self.df_pop_group = (df.merge(self.df_baseline_pop, how='left', on=baseline_vars)\n",
    "    #                        .set_index(df.index)\n",
    "    #                        .loc[:, 'pop_group'])\n",
    "    \n",
    "    # Print information\n",
    "    if verb > 0:\n",
    "      print('Event type : {}, Disease : {}'.format(self.event_type, self.disease))\n",
    "      print('Features used in fit (CoxPH, DeepSurv) : {}'.format(self.feat_X.tolist()))\n",
    "      print('* Train set:'.format())\n",
    "      print('  - length : {}'.format(self.X_train.shape[0]))\n",
    "      print('  - num. events : {}'.format(self.E_train.sum()))\n",
    "      print('* Test set:'.format())\n",
    "      print('  - length : {}'.format(self.X_test.shape[0]))\n",
    "      print('  - num. events : {}'.format(self.E_test.sum()))\n",
    "      \n",
    "    return\n",
    "\n",
    "  def preprocess_data(self, df, dummies='transform', scale='transform', na_replace='transform'):\n",
    "    X = df.loc[:, self.feat]\n",
    "\n",
    "    # Log-transformations\n",
    "    X.loc[:, self.log_feat] = np.log(X.loc[:, self.log_feat])\n",
    "    \n",
    "    # NAs\n",
    "    #from IPython.core.debugger import set_trace; set_trace()\n",
    "    \n",
    "        #### multiple imputations ###\n",
    "    if na_replace is 'fit_transform':\n",
    "        #na_replace = IterativeImputer(max_iter=10, random_state=0, verbose = 2)\n",
    "        na_replace = IterativeImputer(max_iter=1, random_state=0, verbose = 2)\n",
    "        #na_replace.fit(X[self.NA_feat])\n",
    "        #na_replace.fit(X)\n",
    "        #na_replace.fit(X.columns.difference(cat_feat))\n",
    "        na_replace.fit(X[X.columns[~X.columns.isin(cat_feat)]])\n",
    "        self.na_replace = na_replace\n",
    "    elif na_replace is 'transform':\n",
    "        na_replace = self.na_replace\n",
    "    \n",
    "    if na_replace is not None:\n",
    "      NA_cols = pd.DataFrame([])\n",
    "      for feat in self.NA_feat:\n",
    "        NAcol = '{}_NA'.format(feat)\n",
    "        NA_cols[NAcol] = X[feat].isna()\n",
    "\n",
    "        #X[NAcol] = X[feat].isna()\n",
    "      #X[self.NA_feat] = na_replace.transform(X[self.NA_feat])\n",
    "      #X_trans = na_replace.transform(X)\n",
    "      #X.loc[:,:] = X_trans\n",
    "    \n",
    "      X[X.columns[~X.columns.isin(cat_feat)]] = na_replace.transform(X[X.columns[~X.columns.isin(cat_feat)]])\n",
    "\n",
    "      #import pdb; pdb.set_trace()\n",
    "      X = pd.concat([X, NA_cols], axis=1)\n",
    "\n",
    "    #### multiple imputations ###\n",
    "    \n",
    "#     if na_replace is 'fit_transform':\n",
    "#       na_replace = X[self.NA_feat].mean(axis=0, skipna=True)\n",
    "#       self.na_replace = na_replace\n",
    "#     elif na_replace is 'transform':\n",
    "#       na_replace = self.na_replace\n",
    "    \n",
    "#     if na_replace is not None:\n",
    "#       for feat in self.NA_feat:\n",
    "#         NAcol = '{}_NA'.format(feat)\n",
    "#         X[NAcol] = X[feat].isna()\n",
    "#         X[feat].mask(X[NAcol], other=na_replace[feat], inplace=True)\n",
    "      \n",
    "    #import pdb; pdb.set_trace()\n",
    "    X.dropna(axis=0, inplace=True)\n",
    "    X.rename(columns=lambda col: 'log_' + col if col in self.log_feat else col, inplace=True)\n",
    "        \n",
    "    # Dummies\n",
    "    if dummies == 'fit_transform':\n",
    "      import pdb; pdb.set_trace()\n",
    "      self.dummy_encoder = OneHotEncoder(categories='auto', drop='first')\n",
    "      self.dummy_encoder.fit(X[self.cat_feat])\n",
    "    if dummies in ['transform', 'fit_transform']:\n",
    "      dummy_colnames = ['{}_{}'.format(feat, val) for i, feat in enumerate(self.cat_feat) for j, val in enumerate(self.dummy_encoder.categories_[i]) if j != self.dummy_encoder.drop_idx_[i]]\n",
    "      X_dummy = pd.DataFrame(self.dummy_encoder.transform(X[self.cat_feat]).toarray(), index=X.index, columns=dummy_colnames)\n",
    "      X = X.assign(**X_dummy)\n",
    "      X.drop(columns=self.cat_feat, inplace=True)\n",
    "\n",
    "    # Scale\n",
    "    if scale == 'fit_transform':\n",
    "      self.scaler = StandardScaler(copy=False, with_mean=True, with_std=True)\n",
    "      X[X.columns] = self.scaler.fit_transform(X)\n",
    "    elif scale == 'transform':\n",
    "      X[X.columns] = self.scaler.transform(X)\n",
    "    \n",
    "\n",
    "    \n",
    "    return X\n",
    "\n",
    "  def get_age_label(self, age):\n",
    "    if age < self.age_bins[0] or age >= self.age_bins[-1]:\n",
    "      return None\n",
    "    return np.digitize(age, self.age_bins)\n",
    "\n",
    "  def get_age_group(self, age):\n",
    "    return self.df_age_group[self.df_age_group == self.get_age_label(age)].index\n",
    "  \n",
    "  def density(self, x, n_neigh=10):\n",
    "    return np.sqrt(self.X_train.shape[1]) / np.sort(np.linalg.norm(self.X_train-x, axis=1))[:n_neigh].mean()\n",
    "\n",
    "  def CoxPH(self, verb=1, check_assumptions=False, c_index_CI=None, **kwargs):\n",
    "    dataset = self.X_train.assign(T=self.T_train, E=self.E_train)\n",
    "    \n",
    "    if not hasattr(self, 'coxph'):\n",
    "      coxph = CoxPHFitter()\n",
    "      coxph.fit(dataset, duration_col='T', event_col='E', show_progress=(verb>0), **kwargs)\n",
    "    else:\n",
    "      coxph = self.coxph['model']\n",
    "    \n",
    "    if c_index_CI in ['Bootstrap', 'Jackknife']:\n",
    "      print('{} computation of c-index on train and test set...'.format(c_index_CI))\n",
    "      c_index_func = c_index_bootstrap if c_index_CI == 'Bootstrap' else c_index_jackknife\n",
    "      c_index_train, c_train_sample, c_train_sigma2, c_train_lower, c_train_upper = c_index_func(self.T_train, self.E_train, coxph.predict_partial_hazard(self.X_train).values.flatten())\n",
    "      c_index_test, c_test_sample, c_test_sigma2, c_test_lower, c_test_upper = c_index_func(self.T_test, self.E_test, coxph.predict_partial_hazard(self.X_test).values.flatten())\n",
    "      c_index_string = ('C-index (train) : {:.4f} ({:.4f}) CI({:.4f}, {:.4f})\\n'\n",
    "                        'C-index (test)  : {:.4f} ({:.4f}) CI({:.4f}, {:.4f})'.format(c_index_train, np.sqrt(c_train_sigma2), c_train_lower, c_train_upper,\n",
    "                                                                                      c_index_test, np.sqrt(c_test_sigma2), c_test_lower, c_test_upper))\n",
    "    else:\n",
    "      c_index_train = concordance_index(self.T_train, -coxph.predict_partial_hazard(self.X_train), self.E_train)\n",
    "      c_index_test = concordance_index(self.T_test, -coxph.predict_partial_hazard(self.X_test).values.flatten(), self.E_test)\n",
    "      c_index_string = 'C-index (train) : {:.4f}\\nC-index (test)  : {:.4f}'.format(c_index_train, c_index_test)\n",
    "\n",
    "    if verb > 0:\n",
    "      coxph.print_summary()\n",
    "      print('')\n",
    "      print(c_index_string)\n",
    "      print('')\n",
    "      coxph.plot()\n",
    "    \n",
    "    if check_assumptions:\n",
    "      coxph.check_assumptions(dataset, advice=True, show_plots=True, p_value_threshold=0.005)\n",
    "\n",
    "    self.coxph = {'model': coxph,\n",
    "                  'c_index_train': c_index_train, 'c_index_test': c_index_test}\n",
    "    \n",
    "    if c_index_CI in ['Bootstrap', 'Jackknife']:\n",
    "      self.coxph['c_train_var'] = c_train_sigma2\n",
    "      self.coxph['c_train_lower'] = c_train_lower\n",
    "      self.coxph['c_train_upper'] = c_train_upper\n",
    "      self.coxph['c_test_var'] = c_test_sigma2\n",
    "      self.coxph['c_test_lower'] = c_test_lower\n",
    "      self.coxph['c_test_upper'] = c_test_upper\n",
    "\n",
    "    return\n",
    "\n",
    "  def DeepSurv(self, structure, resume_training=True, **kwargs):\n",
    "    # Check if can continue training or need to start from scratch\n",
    "    start_from_scratch = False\n",
    "    if not (hasattr(self, 'deep_surv') and resume_training):\n",
    "      start_from_scratch = True\n",
    "    elif not self.deep_surv.is_structure(structure):\n",
    "      start_from_scratch = True\n",
    "\n",
    "    # Initialize the model (if requested) and fit\n",
    "    if start_from_scratch:\n",
    "      print('Fitting new DeepSurv model...')\n",
    "      self.deep_surv = DeepSurv(structure=structure, \n",
    "                                input_dim=len(self.feat_X), \n",
    "                                n_outcomes=1+len(self.secondary_outcomes))\n",
    "    else:\n",
    "      print('Continuing fit of pre-existing model...')\n",
    "\n",
    "    E_train = (self.E_train,) + self.E_train_II\n",
    "    T_train = (self.T_train,) + self.T_train_II\n",
    "    E_test = (self.E_test,) + self.E_test_II\n",
    "    T_test = (self.T_test,) + self.T_test_II\n",
    "    \n",
    "    self.deep_surv.fit(self.X_train, E_train, T_train,\n",
    "                       self.X_test, E_test, T_test,\n",
    "                       **kwargs)\n",
    "    \n",
    "    # Compute the average risk on Train Set\n",
    "    # def compute_mean_risk(bg):\n",
    "    #   indices_bg = np.intersect1d(self.X_train.index,\n",
    "    #                               self.df_pop_group[self.df_pop_group == bg].index)\n",
    "    #   return self.deep_surv.predict_risk(self.X_train.loc[indices_bg]).mean(axis=0)\n",
    "    \n",
    "    # self.df_baseline_pop['mean_risk'] = self.df_baseline_pop['pop_group'].apply(compute_mean_risk)\n",
    "    self.mean_risk = self.deep_surv.predict_risk(self.X_train).mean(axis=0)\n",
    "\n",
    "    return\n",
    "\n",
    "  def compute_RiskScore(self, df, outcomes=None, compute_d=False, preprocess_data=True):\n",
    "    X = self.preprocess_data(df) if preprocess_data else df\n",
    "    risk_score = self.deep_surv.predict_risk(X) / self.mean_risk\n",
    "    if outcomes is not None:\n",
    "      risk_score = risk_score[:, outcomes]\n",
    "    \n",
    "    if compute_d:\n",
    "      d = X.apply(self.density, n_neigh=10, axis=1).to_numpy()\n",
    "      return risk_score, d\n",
    "\n",
    "    return risk_score\n",
    "\n",
    "  def save(self, filename='RS', folder='.'):\n",
    "    if hasattr(self, 'deep_surv'):\n",
    "      folder_model = '{}/{}_DeepSurv_model'.format(folder, filename)\n",
    "      self.deep_surv.model.save(folder_model,\n",
    "                                overwrite=True, include_optimizer=True,\n",
    "                                save_format='tf')\n",
    "      self.deep_surv.model = None\n",
    "      \n",
    "    pkl_filename = '{}/{}.pkl'.format(folder, filename)\n",
    "    with open(pkl_filename, 'wb') as pkl_file:\n",
    "      dill.dump(self, pkl_file)\n",
    "      print('Dumped file: {}'.format(pkl_filename))\n",
    "    \n",
    "    if hasattr(self, 'deep_surv'):\n",
    "      self.deep_surv.model = tf.keras.models.load_model(folder_model)\n",
    "      \n",
    "    with tarfile.open('{}/{}.tar.gz'.format(folder, filename), mode='w:gz') as tar_archive:\n",
    "      tar_archive.add(pkl_filename)\n",
    "      os.remove(pkl_filename)\n",
    "      if hasattr(self, 'deep_surv'):\n",
    "        tar_archive.add(folder_model)\n",
    "        shutil.rmtree(folder_model)\n",
    "    \n",
    "    print('Successfully saved. The .tar.gz archive can now be downloaded from the file menu on the left.')\n",
    "\n",
    "  @classmethod\n",
    "  def load(cls):\n",
    "    print('Please upload the .tar.gz file:')\n",
    "    uploaded = files.upload()\n",
    "    archive_name = list(uploaded.keys())[0]\n",
    "    while not tarfile.is_tarfile(archive_name):\n",
    "      print('Uploaded file is not a .tar archive. Please upload the .tar.gz file:')\n",
    "      uploaded = files.upload()\n",
    "      archive_name = list(uploaded.keys())[0]\n",
    "    \n",
    "    filename = archive_name[:-7]\n",
    "    with tarfile.open('./{}'.format(archive_name), mode='r') as tar_archive:\n",
    "      tar_archive.extractall()\n",
    "    os.remove(archive_name)\n",
    "    \n",
    "    pkl_filename = './{}.pkl'.format(filename)\n",
    "    with open(pkl_filename, 'rb') as pkl_file:\n",
    "      RS_load = dill.load(pkl_file)\n",
    "      print('Successfully loaded file: {}'.format(pkl_filename))\n",
    "      os.remove(pkl_filename)\n",
    "    \n",
    "    if hasattr(RS_load, 'deep_surv'):\n",
    "      folder_model = './{}_DeepSurv_model'.format(filename)\n",
    "      RS_load.deep_surv.model = tf.keras.models.load_model(folder_model)\n",
    "      print('Successfully uploaded DeepSurv model from folder: {}'.format(folder_model))\n",
    "      shutil.rmtree(folder_model)\n",
    "      \n",
    "    return RS_load\n",
    "\n",
    "\n",
    "def RiskScoreDashboard(RS, patient_data=None):\n",
    "  if not hasattr(RS, 'deep_surv'):\n",
    "    print('No DeepSurv model has been fitted yet. Please use the DeepSurv() function to fit a Neural Network.')\n",
    "    return\n",
    "  \n",
    "  display(HTML('<font size=\"5\">Risk Score Dashboard:</font><br><br>'))\n",
    "  display(HTML('<font size=\"4\">Please input all the patient data:</font><br><br>'))\n",
    "    \n",
    "  age_input = widgets.FloatText(value=patient_data['age_assessment'] if patient_data is not None else None,\n",
    "                                description='Age: ')\n",
    "  sex_input = widgets.RadioButtons(value=patient_data['sex'] if patient_data is not None else None,\n",
    "                                   options=['Male', 'Female'],\n",
    "                                   description='Sex: ')\n",
    "  ethnicity_default = patient_data['ethnicity'] if patient_data is not None else None\n",
    "  if ethnicity_default == 'SouthAsian':\n",
    "    ethnicity_default = 'South Asian' \n",
    "  ethnicity_input = widgets.RadioButtons(value=ethnicity_default,\n",
    "                                         options=['White', 'Black', 'South Asian', 'Other'],\n",
    "                                         description='Ethnicity: ')\n",
    "  \n",
    "  CVD_input = widgets.Checkbox(value=bool(int(patient_data['baseline_CVD'])) if patient_data is not None else None,\n",
    "                               description='Diagnosed CVD', \n",
    "                               style={'description_width': 'initial'})\n",
    "  cancer_input = widgets.Checkbox(value=bool(int(patient_data['baseline_Cancer'])) if patient_data is not None else None,\n",
    "                                  description='Diagnosed cancer', \n",
    "                                  style={'description_width': 'initial'})\n",
    "  hypertension_input = widgets.Checkbox(value=bool(int(patient_data['baseline_Hypertension'])) if patient_data is not None else None,\n",
    "                                        description='Diagnosed hypertension', \n",
    "                                        style={'description_width': 'initial'})\n",
    "  diabetes_input = widgets.Checkbox(value=bool(int(patient_data['baseline_Diabetes'])) if patient_data is not None else None,\n",
    "                                    description='Diagnosed diabetes', \n",
    "                                    style={'description_width': 'initial'})\n",
    "  med_cholesterol_input = widgets.Checkbox(value=bool(int(patient_data['med_cholesterol'])) if patient_data is not None else None,\n",
    "                                           description='Actually on statins', \n",
    "                                           style={'description_width': 'initial'})\n",
    "\n",
    "  smoking_values = ['Never', 'Former', 'Current']\n",
    "  smoking_default = smoking_values[int(patient_data['smoking_num'])] if patient_data is not None else None\n",
    "  smoking_input = widgets.RadioButtons(value=smoking_default,\n",
    "                                       options=smoking_values,\n",
    "                                       description='Smoking status: ', \n",
    "                                       style={'description_width': 'initial'})\n",
    "  alcohol_values = ['Never', 'Special occasions only', 'One to three times a month', 'Once or twice a week', 'Three or four times a week', 'Daily or almost daily']\n",
    "  alcohol_default = alcohol_values[int(patient_data['alcohol_num'])] if patient_data is not None else None\n",
    "  alcohol_input = widgets.RadioButtons(value=alcohol_default,\n",
    "                                       options=alcohol_values,\n",
    "                                       description='Alcohol consumed: ', \n",
    "                                       style={'description_width': 'initial'})\n",
    "  diet_values = ['Ideal', 'Poor']\n",
    "  diet_default = diet_values[int((patient_data['diet_HI']-1)/2)] if patient_data is not None else None\n",
    "  diet_input = widgets.RadioButtons(value=diet_default,\n",
    "                                    options=diet_values,\n",
    "                                    description='Diet: ', \n",
    "                                    style={'description_width': 'initial'})\n",
    "  moderate_activity_input = widgets.FloatText(value=patient_data['moderate_activity'] if patient_data is not None else None,\n",
    "                                              description='Moderate Physical activity (mins/week): ', \n",
    "                                              style={'description_width': 'initial'})\n",
    "  vigorous_activity_input = widgets.FloatText(value=patient_data['vigorous_activity'] if patient_data is not None else None,\n",
    "                                              description='Vigorous Physical activity (mins/week): ', \n",
    "                                              style={'description_width': 'initial'})\n",
    "  BMI_input = widgets.FloatText(value=patient_data['BMI'] if patient_data is not None else None,\n",
    "                                description='BMI (kg/m2): ', \n",
    "                                style={'description_width': 'initial'})\n",
    "  diastolic_blood_pressure_input = widgets.FloatText(value=patient_data['diastolic_blood_pressure'] if patient_data is not None else None,\n",
    "                                                     description='Diastolic Blood Pressure (mmHg): ', \n",
    "                                                     style={'description_width': 'initial'})\n",
    "  systolic_blood_pressure_input = widgets.FloatText(value=patient_data['systolic_blood_pressure'] if patient_data is not None else None,\n",
    "                                                    description='Systolic Blood Pressure (mmHg): ', \n",
    "                                                    style={'description_width': 'initial'})\n",
    "  \n",
    "  cholesterol_input = widgets.FloatText(value=patient_data['cholesterol'] if patient_data is not None else None,\n",
    "                                        description='Cholesterol (mmol/L): ', \n",
    "                                        style={'description_width': 'initial'})\n",
    "  LDL_input = widgets.FloatText(value=patient_data['LDL'] if patient_data is not None else None,\n",
    "                                description='LDL (mmol/L): ', \n",
    "                                style={'description_width': 'initial'})\n",
    "  HDL_input = widgets.FloatText(value=patient_data['HDL'] if patient_data is not None else None,\n",
    "                                description='HDL (mmol/L): ', \n",
    "                                style={'description_width': 'initial'})\n",
    "  triglycerides_input = widgets.FloatText(value=patient_data['triglycerides'] if patient_data is not None else None,\n",
    "                                          description='Triglycerides (mmol/L): ', \n",
    "                                          style={'description_width': 'initial'})\n",
    "  ApoA_input = widgets.FloatText(value=patient_data['ApoA'] if patient_data is not None else None,\n",
    "                                 description='ApoA (g/L): ', \n",
    "                                 style={'description_width': 'initial'})\n",
    "  ApoB_input = widgets.FloatText(value=patient_data['ApoB'] if patient_data is not None else None,\n",
    "                                 description='ApoB (g/L): ', \n",
    "                                 style={'description_width': 'initial'})\n",
    "  CRP_input = widgets.FloatText(value=patient_data['CRP'] if patient_data is not None else None,\n",
    "                                description='CRP (mg/L): ', \n",
    "                                style={'description_width': 'initial'})\n",
    "  lipoproteinA_input = widgets.FloatText(value=patient_data['lipoproteinA'] if patient_data is not None else None,\n",
    "                                         description='Lipoprotein A (nmol/L): ', \n",
    "                                         style={'description_width': 'initial'})\n",
    "  urate_input = widgets.FloatText(value=patient_data['urate'] if patient_data is not None else None,\n",
    "                                  description='Urate (umol/L): ', \n",
    "                                  style={'description_width': 'initial'})\n",
    "\n",
    "  display(age_input)\n",
    "  display(sex_input)\n",
    "  display(ethnicity_input)\n",
    "\n",
    "  display(CVD_input)\n",
    "  display(cancer_input)\n",
    "  display(hypertension_input)\n",
    "  display(diabetes_input)\n",
    "  display(med_cholesterol_input)\n",
    "\n",
    "  display(smoking_input)\n",
    "  display(alcohol_input)\n",
    "  display(diet_input)\n",
    "  display(moderate_activity_input)\n",
    "  display(vigorous_activity_input)\n",
    "  display(BMI_input)\n",
    "  display(diastolic_blood_pressure_input)\n",
    "  display(systolic_blood_pressure_input)\n",
    "\n",
    "  display(cholesterol_input)\n",
    "  display(LDL_input)\n",
    "  display(HDL_input)\n",
    "  display(triglycerides_input)\n",
    "  display(ApoA_input)\n",
    "  display(ApoB_input)\n",
    "  display(CRP_input)\n",
    "  display(lipoproteinA_input)\n",
    "  display(urate_input)\n",
    "  print('')\n",
    "\n",
    "  compute_button = widgets.Button(description='Compute Risk Score',\n",
    "                                  button_style='info',# 'success', 'info', 'warning', 'danger' or ''\n",
    "                                  tooltip='Compute the Risk Score for the patient with values inserted',\n",
    "                                  icon='heartbeat')#stethoscope\n",
    "  display(compute_button)\n",
    "  out = widgets.Output()\n",
    "\n",
    "  def output_score(b):\n",
    "    df_patient = pd.DataFrame(data={'age_assessment': [age_input.value],\n",
    "                                    'sex': [sex_input.value],\n",
    "                                    'ethnicity': [ethnicity_input.value.replace(' ', '')],\n",
    "                                    'baseline_CVD': [CVD_input.value],\n",
    "                                    'baseline_Cancer': [cancer_input.value],\n",
    "                                    'baseline_Hypertension': [hypertension_input.value],\n",
    "                                    'baseline_Diabetes': [diabetes_input.value],\n",
    "                                    'med_cholesterol': [med_cholesterol_input.value],\n",
    "                                    'smoking_num': [smoking_values.index(smoking_input.value)],\n",
    "                                    'alcohol_num': [alcohol_values.index(alcohol_input.value)],\n",
    "                                    'diet_HI': [2*diet_values.index(diet_input.value) + 1],\n",
    "                                    'moderate_activity': [moderate_activity_input.value],\n",
    "                                    'vigorous_activity': [vigorous_activity_input.value],\n",
    "                                    'BMI': [BMI_input.value],\n",
    "                                    'diastolic_blood_pressure': [diastolic_blood_pressure_input.value],\n",
    "                                    'systolic_blood_pressure': [systolic_blood_pressure_input.value],\n",
    "                                    'cholesterol': [cholesterol_input.value],\n",
    "                                    'LDL': [LDL_input.value],\n",
    "                                    'HDL': [HDL_input.value],\n",
    "                                    'triglycerides': [triglycerides_input.value],\n",
    "                                    'ApoA': [ApoA_input.value],\n",
    "                                    'ApoB': [ApoB_input.value],\n",
    "                                    'CRP': [CRP_input.value],\n",
    "                                    'lipoproteinA': [lipoproteinA_input.value],\n",
    "                                    'urate': [urate_input.value]})\n",
    "  \n",
    "    risk_scores = RS.compute_RiskScore(df_patient, outcomes=None, compute_d=False, preprocess_data=True).flatten()\n",
    "    \n",
    "    outcome_str = ['Incidence of Composite CHD', 'Incidence of (All-type) Cancer', 'All-cause Mortality']\n",
    "\n",
    "    with out:\n",
    "      out.clear_output()\n",
    "      display(HTML('<br><font size=\"4\"><b>Risk Scores:</b></font><br><br>'))\n",
    "      table = '<font size=\"4\"><table>'\n",
    "      for i, risk_i in enumerate(risk_scores):\n",
    "        table += '<tr><th>{}.</th><td>Risk Score for <i>{}</i>:</td> <td>{:.2f}</td></tr>'.format(i+1, outcome_str[i], risk_i)\n",
    "      table += '</table></font>'\n",
    "      display(HTML(table))\n",
    "\n",
    "    return\n",
    "\n",
    "  compute_button.on_click(output_score)\n",
    "  display(out)\n",
    "\n",
    "  return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twelve-public",
   "metadata": {},
   "source": [
    "# ALL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "opening-compression",
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_feat = ['alanine_aminotransferase', 'albumin', 'alkaline_phosphatase', 'ApoA', 'ApoB',\n",
    "'aspartate_aminotransferase', 'CRP', 'calcium', 'cholesterol', 'creatinine', \n",
    "'cystatinC', 'direct_bilirubin', 'gamma_glutamyltransferase', 'glucose', \n",
    "'glycated_haemoglobin', 'HDL', 'IGF1', 'LDL', 'lipoproteinA', 'oestradiol',\n",
    " 'phosphate', 'rheumatoid_factor', 'SHBG', 'testosterone', 'total_bilirubin',\n",
    " 'total_protein', 'triglycerides', 'urate', 'urea', 'vitaminD', 'ApoB_ApoA_ratio', \n",
    "  'nonHDL', 'LDL_Friedewald', 'LDL_MartinHopkins']\n",
    "\n",
    "all_feat = list(set(['sex', 'age_assessment', 'ethnicity',\n",
    "            'systolic_blood_pressure', 'diastolic_blood_pressure',\n",
    "            'smoking_num', 'smoking_cat', 'alcohol_num', 'alcohol_cat', 'BMI', 'moderate_activity', 'vigorous_activity',\n",
    "            'diet_HI', 'physical_activity_HI', 'BMI_HI', 'smoking_HI', 'HLI',\n",
    "            'baseline_Hypertension', 'baseline_Diabetes', 'baseline_CVD',\n",
    "            'med_cholesterol', 'med_insuline', 'med_blood_pressure',\n",
    "            'glucose', 'creatinine', 'CRP',\n",
    "            'cholesterol', 'triglycerides', 'HDL', 'nonHDL', 'LDL', 'LDL_Friedewald', 'LDL_MartinHopkins', 'dyslipidemia',\n",
    "            'ApoA', 'ApoB', 'ApoB_ApoA_ratio', 'lipoproteinA', 'urate', 'IGF1', 'albumin', 'glycated_haemoglobin',\n",
    "            'testosterone', 'oestradiol', 'SHBG'] + lab_feat))\n",
    "#cat_feat = ['sex', 'ethnicity', 'smoking_cat', 'alcohol_cat']\n",
    "#log_feat = ['BMI', 'CRP',\n",
    "#            'ApoA', 'ApoB', 'ApoB_ApoA_ratio', 'lipoproteinA',\n",
    "#            'HDL', 'LDL', 'LDL_Friedewald', 'LDL_MartinHopkins', 'cholesterol', 'triglycerides',\n",
    "#            'glucose', 'creatinine', 'urate', 'IGF1', 'albumin', 'glycated_haemoglobin',\n",
    "#            'testosterone', 'oestradiol', 'SHBG']\n",
    "log_feat = ['BMI'] + lab_feat + ['systolic_blood_pressure', 'diastolic_blood_pressure']\n",
    "\n",
    "risk_factors = ['age_assessment', 'sex'] + ['systolic_blood_pressure'] + ['ethnicity', 'diastolic_blood_pressure']\n",
    "HI_feat = ['diet_HI', 'moderate_activity', 'vigorous_activity', 'BMI', 'smoking_num', 'alcohol_num']\n",
    "lipid_feat = ['cholesterol', 'lipoproteinA', 'triglycerides', 'HDL', 'LDL', 'ApoA', 'ApoB', 'CRP', 'urate',\n",
    "              'med_cholesterol']\n",
    "baseline_feat = ['baseline_CVD', 'baseline_Cancer', 'baseline_Hypertension', 'baseline_Diabetes',\n",
    "                'med_cholesterol', 'med_insuline', 'med_blood_pressure']\n",
    "NA_feat = [feat for feat in all_feat if df[feat].isna().sum()/df.shape[0]*100 >= 10]\n",
    "\n",
    "model_feat = risk_factors + HI_feat + lipid_feat\n",
    "\n",
    "\n",
    "#model_feat = [\n",
    "#            'glucose', 'creatinine', 'CRP',\n",
    "#            'cholesterol', 'triglycerides', 'HDL', 'nonHDL', 'LDL', 'LDL_Friedewald', 'LDL_MartinHopkins',\n",
    "#            'ApoA', 'ApoB', 'ApoB_ApoA_ratio', 'lipoproteinA', 'urate', 'IGF1', 'albumin', 'glycated_haemoglobin',\n",
    "#            'testosterone', 'oestradiol', 'SHBG'] + risk_factors + HI_feat\n",
    "\n",
    "\n",
    "#model_feat = lab_feat + risk_factors + HI_feat + baseline_feat\n",
    "#model_feat = all_feat\n",
    "#model_feat = lab_feat + risk_factors + HI_feat\n",
    "model_feat = all_feat\n",
    "\n",
    "cat_feat = [feat for feat in model_feat if df[feat].dtypes == 'object']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "lasting-blink",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ApoA', 'ApoB', 'ApoB_ApoA_ratio', 'BMI', 'BMI_HI', 'CRP',\n",
       "       'E_Incidence_CHD', 'E_Incidence_CVD', 'E_Incidence_Cancer',\n",
       "       'E_Incidence_Composite', 'E_Incidence_Stroke', 'E_Incidence_W-CVD',\n",
       "       'E_Mortality_All', 'E_Mortality_CHD', 'E_Mortality_CVD',\n",
       "       'E_Mortality_Cancer', 'E_Mortality_Composite', 'E_Mortality_Stroke',\n",
       "       'E_Mortality_W-CVD', 'HDL', 'HLI', 'IGF1', 'LDL', 'LDL_Friedewald',\n",
       "       'LDL_MartinHopkins', 'SHBG', 'T_Incidence_CHD', 'T_Incidence_CVD',\n",
       "       'T_Incidence_Cancer', 'T_Incidence_Composite', 'T_Incidence_Stroke',\n",
       "       'T_Incidence_W-CVD', 'T_Mortality_All', 'T_Mortality_CHD',\n",
       "       'T_Mortality_CVD', 'T_Mortality_Cancer', 'T_Mortality_Composite',\n",
       "       'T_Mortality_Stroke', 'T_Mortality_W-CVD', 'age_assessment',\n",
       "       'age_death', 'alanine_aminotransferase', 'albumin', 'alcohol_num',\n",
       "       'alkaline_phosphatase', 'aspartate_aminotransferase', 'baseline_CVD',\n",
       "       'baseline_Cancer', 'baseline_Diabetes', 'baseline_Hypertension',\n",
       "       'calcium', 'cholesterol', 'creatinine', 'cystatinC', 'date_assessment',\n",
       "       'date_birth', 'diastolic_blood_pressure', 'diet_HI', 'direct_bilirubin',\n",
       "       'dyslipidemia', 'gamma_glutamyltransferase', 'glucose',\n",
       "       'glycated_haemoglobin', 'lipoproteinA', 'med_blood_pressure',\n",
       "       'med_cholesterol', 'med_insuline', 'moderate_activity', 'nonHDL',\n",
       "       'oestradiol', 'phosphate', 'physical_activity_HI', 'rheumatoid_factor',\n",
       "       'smoking_HI', 'smoking_num', 'systolic_blood_pressure', 'testosterone',\n",
       "       'total_bilirubin', 'total_protein', 'triglycerides', 'urate', 'urea',\n",
       "       'vigorous_activity', 'vitaminD'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns.difference(cat_feat)\n",
    "#df[model_feat].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "straight-haiti",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[IterativeImputer] Completing matrix with shape (452293, 54)\n"
     ]
    }
   ],
   "source": [
    "RS = RiskScore(df,\n",
    "               features=model_feat,\n",
    "               log_features=log_feat, NA_features=NA_feat, cat_features=cat_feat,\n",
    "               test_size=0.1)\n",
    "\n",
    "activ = lambda x: tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n",
    "l2reg = 1e-4\n",
    "do = 0.1\n",
    "bn = True\n",
    "\n",
    "structure = {'inner_layers': [{'num_units': 120, 'activation': activ, 'l2_reg': l2reg, 'batch_norm': bn, 'dropout': do},\n",
    "                              {'num_units': 80, 'activation': activ, 'l2_reg': l2reg, 'batch_norm': bn, 'dropout': do},\n",
    "                              {'num_units': 80, 'activation': activ, 'l2_reg': l2reg, 'batch_norm': bn, 'dropout': do},\n",
    "                              {'num_units': 20, 'activation': activ, 'l2_reg': l2reg, 'batch_norm': bn, 'dropout': 0}],\n",
    "             'output_layer': {'l2_reg': l2reg,\n",
    "                              'use_bias': False}}\n",
    "\n",
    "RS.DeepSurv(structure, resume_training=True,\n",
    "            optimizer=tf.keras.optimizers.Adam(),\n",
    "            n_epochs=60, lr=1e-4, batch_size=None, batch_n_events=100,\n",
    "            evaluation_prms={'loss_test': True,\n",
    "                             'c_test': True, 'c_train': True, 'n_bootstrap': 10,\n",
    "                             'check_KM': False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "freelance-surgeon",
   "metadata": {},
   "outputs": [],
   "source": [
    "['med_cholesterol',\n",
    " 'age_assessment',\n",
    " 'sex_Male',\n",
    " 'smoking_num',\n",
    " 'log_urate',\n",
    " 'systolic_blood_pressure',\n",
    " 'log_HDL',\n",
    " 'log_CRP',\n",
    " 'log_ApoA',\n",
    " 'log_triglycerides',\n",
    " 'log_BMI',\n",
    " 'vigorous_activity_NA',\n",
    " 'log_cholesterol',\n",
    " 'alcohol_num',\n",
    " 'log_lipoproteinA',\n",
    " 'diastolic_blood_pressure',\n",
    " 'ethnicity_SouthAsian',\n",
    " 'log_LDL',\n",
    " 'moderate_activity_NA',\n",
    " 'moderate_activity',\n",
    " 'lipoproteinA_NA',\n",
    " 'ethnicity_Other',\n",
    " 'vigorous_activity',\n",
    " 'log_ApoB',\n",
    " 'diet_HI',\n",
    " 'diet_HI_NA',\n",
    " 'ethnicity_White',\n",
    " 'ApoA_NA',\n",
    " 'HDL_NA']\n",
    "\n",
    "lab_feat = ['alanine_aminotransferase', 'albumin', 'alkaline_phosphatase', 'ApoA', 'ApoB',\n",
    "'aspartate_aminotransferase', 'CRP', 'calcium', 'cholesterol', 'creatinine', \n",
    "'cystatinC', 'direct_bilirubin', 'gamma_glutamyltransferase', 'glucose', \n",
    "'glycated_haemoglobin', 'HDL', 'IGF1', 'LDL', 'lipoproteinA', 'oestradiol',\n",
    " 'phosphate', 'rheumatoid_factor', 'SHBG', 'testosterone', 'total_bilirubin',\n",
    " 'total_protein', 'triglycerides', 'urate', 'urea', 'vitaminD', 'ApoB_ApoA_ratio', \n",
    "  'nonHDL', 'LDL_Friedewald', 'LDL_MartinHopkins']\n",
    "\n",
    "all_feat = list(set(['sex', 'age_assessment', 'ethnicity',\n",
    "            'systolic_blood_pressure', 'diastolic_blood_pressure',\n",
    "            'smoking_num', 'smoking_cat', 'alcohol_num', 'alcohol_cat', 'BMI', 'moderate_activity', 'vigorous_activity',\n",
    "            'diet_HI', 'physical_activity_HI', 'BMI_HI', 'smoking_HI', 'HLI',\n",
    "            'baseline_Hypertension', 'baseline_Diabetes', 'baseline_CVD',\n",
    "            'med_cholesterol', 'med_insuline', 'med_blood_pressure',\n",
    "            'glucose', 'creatinine', 'CRP',\n",
    "            'cholesterol', 'triglycerides', 'HDL', 'nonHDL', 'LDL', 'LDL_Friedewald', 'LDL_MartinHopkins', 'dyslipidemia',\n",
    "            'ApoA', 'ApoB', 'ApoB_ApoA_ratio', 'lipoproteinA', 'urate', 'IGF1', 'albumin', 'glycated_haemoglobin',\n",
    "            'testosterone', 'oestradiol', 'SHBG'] + lab_feat))\n",
    "cat_feat = ['sex', 'ethnicity', 'smoking_cat', 'alcohol_cat']\n",
    "log_feat = ['BMI', 'CRP',\n",
    "            'ApoA', 'ApoB', 'ApoB_ApoA_ratio', 'lipoproteinA',\n",
    "            'HDL', 'LDL', 'LDL_Friedewald', 'LDL_MartinHopkins', 'cholesterol', 'triglycerides',\n",
    "            'glucose', 'creatinine', 'urate', 'IGF1', 'albumin', 'glycated_haemoglobin',\n",
    "            'testosterone', 'oestradiol', 'SHBG']\n",
    "log_feat = ['BMI'] + lab_feat\n",
    "\n",
    "risk_factors = ['age_assessment', 'sex'] + ['systolic_blood_pressure'] + ['ethnicity', 'diastolic_blood_pressure']\n",
    "HI_feat = ['diet_HI', 'moderate_activity', 'vigorous_activity', 'BMI', 'smoking_num', 'alcohol_num']\n",
    "lipid_feat = ['cholesterol', 'lipoproteinA', 'triglycerides', 'HDL', 'LDL', 'ApoA', 'ApoB', 'CRP', 'urate',\n",
    "              'med_cholesterol']\n",
    "baseline_feat = ['baseline_CVD', 'baseline_Cancer', 'baseline_Hypertension', 'baseline_Diabetes',\n",
    "                'med_cholesterol', 'med_insuline', 'med_blood_pressure']\n",
    "NA_feat = [feat for feat in all_feat if df[feat].isna().sum()/df.shape[0]*100 >= 10]\n",
    "\n",
    "model_feat = risk_factors + HI_feat + lipid_feat\n",
    "\n",
    "\n",
    "model_feat = [\n",
    "            'glucose', 'creatinine', 'CRP',\n",
    "            'cholesterol', 'triglycerides', 'HDL', 'nonHDL', 'LDL', 'LDL_Friedewald', 'LDL_MartinHopkins',\n",
    "            'ApoA', 'ApoB', 'ApoB_ApoA_ratio', 'lipoproteinA', 'urate', 'IGF1', 'albumin', 'glycated_haemoglobin',\n",
    "            'testosterone', 'oestradiol', 'SHBG'] + risk_factors + HI_feat\n",
    "#model_feat = lab_feat + risk_factors + HI_feat + baseline_feat\n",
    "model_feat = all_feat\n",
    "#model_feat = lab_feat + risk_factors + HI_feat\n",
    "#model_feat = lab_feat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anonymous-poster",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[df.columns[~df.columns.isin(cat_feat)]].isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "union-nightmare",
   "metadata": {},
   "outputs": [],
   "source": [
    "RS_composite_all = RiskScore(df,\n",
    "                               features=model_feat,\n",
    "                               log_features=log_feat, NA_features=NA_feat, cat_features=cat_feat,\n",
    "                             test_size=0.1)\n",
    "\n",
    "activ = lambda x: tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n",
    "l2reg = 1e-4\n",
    "do = 0.1\n",
    "bn = True\n",
    "\n",
    "#structure = {'inner_layers': [{'num_units': 120, 'activation': activ, 'l2_reg': l2reg, 'batch_norm': bn, 'dropout': do},\n",
    "#                              {'num_units': 80, 'activation': activ, 'l2_reg': l2reg, 'batch_norm': bn, 'dropout': do},\n",
    "#                              {'num_units': 80, 'activation': activ, 'l2_reg': l2reg, 'batch_norm': bn, 'dropout': do},\n",
    "#                              {'num_units': 20, 'activation': activ, 'l2_reg': l2reg, 'batch_norm': bn, 'dropout': 0}],\n",
    "#             'output_layer': {'l2_reg': l2reg,\n",
    "#                              'use_bias': False}}\n",
    "\n",
    "structure = {'inner_layers': [{'num_units': 240, 'activation': activ, 'l2_reg': l2reg, 'batch_norm': bn, 'dropout': do},\n",
    "                              {'num_units': 160, 'activation': activ, 'l2_reg': l2reg, 'batch_norm': bn, 'dropout': do},\n",
    "                              {'num_units': 160, 'activation': activ, 'l2_reg': l2reg, 'batch_norm': bn, 'dropout': do},\n",
    "                              {'num_units': 40, 'activation': activ, 'l2_reg': l2reg, 'batch_norm': bn, 'dropout': 0}],\n",
    "             'output_layer': {'l2_reg': l2reg,\n",
    "                              'use_bias': False}}\n",
    "\n",
    "RS_composite_all.DeepSurv(structure, resume_training=True,\n",
    "            optimizer=tf.keras.optimizers.Adam(),\n",
    "            n_epochs=60, lr=1e-4, batch_size=None, batch_n_events=100,\n",
    "            evaluation_prms={'loss_test': True,\n",
    "                             'c_test': True, 'c_train': True, 'n_bootstrap': 10,\n",
    "                             'check_KM': False})\n",
    "\n",
    "RS_composite_all.save(filename = 'RS_composite_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lyric-hepatitis",
   "metadata": {},
   "outputs": [],
   "source": [
    "###RS_composite_all\n",
    "\n",
    "sub_dataset_all = RS_composite_all.X_train.assign(T=RS_composite_all.T_train, E=RS_composite_all.E_train)\n",
    "sub_dataset_all\n",
    "\n",
    "sub_coxph_all = CoxPHFitter(penalizer = 0.1)\n",
    "sub_coxph_all.fit(sub_dataset_all, duration_col='T', event_col='E')\n",
    "sub_coxph_all.print_summary()\n",
    "\n",
    "c_index_train, c_train_sample, c_train_sigma2, c_train_lower, c_train_upper = c_index_bootstrap(RS_composite_all.T_train, RS_composite_all.E_train, sub_coxph_all.predict_partial_hazard(RS_composite_all.X_train).values.flatten())\n",
    "c_index_test, c_test_sample, c_test_sigma2, c_test_lower, c_test_upper = c_index_bootstrap(RS_composite_all.T_test, RS_composite_all.E_test, sub_coxph_all.predict_partial_hazard(RS_composite_all.X_test).values.flatten())\n",
    "c_index_string = ('C-index (train) : {:.4f} ({:.4f}) CI({:.4f}, {:.4f})\\n'\n",
    "                  'C-index (test)  : {:.4f} ({:.4f}) CI({:.4f}, {:.4f})'.format(c_index_train, np.sqrt(c_train_sigma2), c_train_lower, c_train_upper,\n",
    "                                                                                c_index_test, np.sqrt(c_test_sigma2), c_test_lower, c_test_upper))\n",
    "print(c_index_string)\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"]=20,30\n",
    "matplotlib.rcParams.update({'font.size': 22})\n",
    "\n",
    "sub_coxph_all.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominican-threshold",
   "metadata": {},
   "outputs": [],
   "source": [
    "RS_composite_all.X_train.to_pickle('RS_composite_all_X_TRAIN')\n",
    "np.savetxt('RS_composite_all.T_train.txt', RS_composite_all.T_train, fmt='%d')\n",
    "np.savetxt('RS_composite_all.E_train.txt', RS_composite_all.E_train, fmt='%d')\n",
    "\n",
    "RS_CVD_all.X_test.to_pickle('RS_composite_all_X_test')\n",
    "np.savetxt('RS_composite_all.T_test.txt', RS_composite_all.T_test, fmt='%d')\n",
    "np.savetxt('RS_composite_all.E_test.txt', RS_composite_all.E_test, fmt='%d')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
